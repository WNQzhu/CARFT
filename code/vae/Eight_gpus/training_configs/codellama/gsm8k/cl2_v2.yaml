# Model arguments
model_init_kwargs:
  model_name_or_path:  /mnt/data/users/wnq/models/reasoning/codellama/gsm8k/sft/checkpoint-97
  model_name: codellama
  attn_implementation: flash_attention_2
  # torch_dtype: null
  torch_dtype: bfloat16
  low_cpu_mem_usage: False

# cot
cot:
  instruction: "Question:\n"
  cot_trigger: "\nAnswer reasoning:\n"
  answer_trigger: "\nTherefore, the answer is: "


# skip
# dataset_kwargs:
#  skip_prepare_dataset: True

# Data training arguments

## engine
engine: 'nl'
train_path: /mnt/data/users/wnq/bank/bundle.rollout/code/mwp_ReFT/data/gsm8k_nl.json
eval_path: /mnt/data/users/wnq/bank/bundle.rollout/code/mwp_ReFT/data/gsm8k_test_set.json
dataset_name: gsm8k

max_length: 1024

bf16: true


do_eval: True
eval_strategy: "epoch"
# eval_strategy: "no"
# eval_steps: 10
# no
# steps
# epoch
# eval_steps: 400
gradient_accumulation_steps: 2
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: False
hub_model_id: cl2-v2-exps
learning_rate: 3.0e-7
log_level: info
logging_steps: 500
packing: false
num_train_epochs: 100
# override num_train_epochs for test
# max_steps: 4
optim: adamw_torch
output_dir: /mnt/data/users/wnq/models/reasoning/codellama/gsm8k/cl2_v2
run_name: codellama-cl2_v2-gsm8k
per_device_train_batch_size: 4
per_device_eval_batch_size: 32
push_to_hub: false
metric_for_best_model: "myacc"
save_strategy: "epoch"
# save_steps: 10
report_to:
- none
save_total_limit: 3
seed: 42
world_size: 8

load_best_model_at_end: True

# cl coef

ddp_timeout: 3600

cl_coef: 0.01

disable_tqdm: True